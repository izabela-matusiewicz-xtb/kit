# Kit AI-Powered PR Reviewer

`kit` includes a production-ready CLI-based pull request reviewer that provides intelligent, comprehensive code reviews using Anthropic or OpenAI models, with cost transparency and professional-grade analysis.

## ğŸš€ Quick Start

```bash
# 1. Install kit
pip install cased-kit

# 2. Initialize configuration
kit review --init-config

# 3. Set your API keys
export KIT_GITHUB_TOKEN="ghp_your_github_token"
export KIT_ANTHROPIC_TOKEN="sk-ant-your_anthropic_key"

# 4. Review any GitHub PR
kit review https://github.com/owner/repo/pull/123

# 5. Test without posting (dry run)
kit review --dry-run https://github.com/owner/repo/pull/123
```

## ğŸ’° Cost Economics

Kit provides **exceptional value** with transparent pricing. Based on real-world testing:

### **Actual Cost Data (Typical 3-6 file PR)**

| Mode | Typical Cost Range | Speed | Use Case |
|------|------------|-------|----------|
| **Standard** | $0.05-0.20 | 15-45 sec | Daily development - **recommended** |
| **Agentic** | $0.36-2.57 | 1-5 min | Research/experimentation |

### **Real-World Monthly Costs**

**Key benefits**:
- No per-seat licensing - costs scale with usage, not team size
- Whole repo context and multiple code tools - all powered by kit
- Complete cost transparency - see exact LLM costs with no markup

### **Complete Pricing Matrix (Real Test Results)**

Tested on a large 12-file PR with comprehensive analysis. Costs are often lower (e.g., sonnet4 on most PRs we get
at 8 cents a review).

| Model | Provider | Cost | Input Tokens | Output Tokens | $/Million Input | $/Million Output | Speed Tier |
|-------|----------|------|--------------|---------------|-----------------|------------------|------------|
| **gpt-4.1-nano** | OpenAI | **$0.0046** | 42,002 | 885 | $0.10 | $0.40 | âš¡ Ultra Budget |
| **gpt-4o-mini** | OpenAI | **$0.0067** | 40,839 | 902 | $0.15 | $0.60 | âš¡ Budget |
| **gpt-4.1-mini** | OpenAI | **$0.0191** | 42,026 | 1,460 | $0.40 | $1.60 | âš¡ Budget |
| **claude-3-5-haiku-20241022** | Anthropic | **$0.0447** | 52,627 | 660 | $0.80 | $4.00 | âš¡ Budget |
| **gpt-4.1** | OpenAI | **$0.1010** | 42,007 | 2,122 | $2.00 | $8.00 | ğŸš€ Good Value |
| **gpt-4o** | OpenAI | **$0.1089** | 40,873 | 672 | $2.50 | $10.00 | ğŸš€ Good Value |
| **claude-sonnet-4-20250514** | Anthropic | **$0.1759** | 52,667 | 1,195 | $3.00 | $15.00 | ğŸš€ **Recommended** |
| **claude-3-5-sonnet-20241022** | Anthropic | **$0.1774** | 52,593 | 1,307 | $3.00 | $15.00 | ğŸš€ Fast |
| **gpt-4-turbo** | OpenAI | **$0.4258** | 40,598 | 659 | $10.00 | $30.00 | ğŸ’° Premium |
| **claude-opus-4-20250514** | Anthropic | **$0.9086** | 52,597 | 1,595 | $15.00 | $75.00 | ğŸ§  Most Capable |

**Key Insights:**
- **197x price difference** between cheapest (GPT-4.1-nano) and most capable (Claude Opus 4)
- **GPT-4.1-nano** offers incredible value at $0.005 per large PR - cheapest available
- **Claude Sonnet 4** (default) balances cost and quality - comprehensive repository analysis
- **OpenAI models** use fewer input tokens (~40k vs ~52k) but deliver comparable analysis depth
- **GPT-4.1 series** provides excellent mid-range options between ultra-budget and premium

> **ğŸ’¡ Pro Tip:** Don't underestimate the smaller models! GPT-4.1-nano and GPT-4o-mini deliver surprisingly useful reviews. For most small teams (20-50 PRs/month), you can get comprehensive AI code reviews for **less than $1/month** with these budget models. That's potentially transformative value for the cost of a coffee.

**Projected Monthly Costs (Based on Real Data):**

| Team Size | GPT-4.1-nano (Ultra Budget) | Claude Sonnet 4 (Recommended) | Claude Opus 4 (Premium) |
|-----------|------------------------------|--------------------------------|--------------------------|
| **Small** (20 PRs/month) | $0.09 | $3.52 | $18.17 |
| **Medium** (100 PRs/month) | $0.46 | $17.59 | $90.86 |
| **Enterprise** (500 PRs/month) | $2.30 | $87.95 | $454.30 |

*Assumes mix of PR sizes with large PRs (12+ files) representing ~30% of reviews*

## ğŸ¯ Review Modes Deep Dive

### Standard Mode (Recommended)

**How it works**: Leverages kit's repository intelligence for comprehensive reviews with symbol analysis and cross-codebase impact assessment.

**Strengths**:
- Excellent quality-to-cost ratio
- Repository context and symbol analysis
- Identifies impact on functions/classes used elsewhere
- Fast single-pass execution
- Professional, actionable feedback

**Example capabilities**:
- Identifies that a function is used in 23 other places
- Suggests specific code improvements with examples
- Analyzes dependency changes and their implications
- Provides architectural guidance based on repository structure

### Agentic Mode (Experimental)

**How it works**: Uses multi-turn analysis where the AI investigates the PR using kit's tools through iterative exploration. Expensive, and with unclear quality improvenents.

**Important Notes**:
- **Experimental feature** - currently serves as research artifact
- Can produce overconfident conclusions about non-existent issues
- Significantly more expensive than other modes
- Demonstrates that complex approaches don't always yield better results

**Best for**: Research, experimentation, understanding current AI agent limitations

## ğŸ¯ Key Features

### Intelligent Analysis
- Deep code understanding using repository cloning and analysis
- Context-aware reviews with full codebase knowledge
- Multi-language support for any language kit supports
- Security and architecture analysis

### Professional Output
- Concise, actionable feedback without unnecessary verbosity
- Priority-based issue ranking (High/Medium/Low)
- Clickable GitHub links for all file references
- Professional tone without dramatic language

### Cost Transparency
- Real-time cost tracking showing exact LLM usage
- Token breakdown (input/output) for understanding cost drivers
- Model information showing which LLM provided the analysis

### Enterprise Features
- Repository caching for faster repeat reviews
- Multiple LLM support (Anthropic Claude, OpenAI GPT-4)
- Configurable analysis depth and turn limits
- GitHub integration with token-based authentication

## ğŸ“‹ Setup Instructions

### 1. GitHub Token Setup

You need a GitHub token with appropriate permissions:

#### Classic Personal Access Token (Recommended)

1. Go to **GitHub â†’ Settings â†’ Developer settings â†’ Personal access tokens â†’ Tokens (classic)**
2. Click **"Generate new token (classic)"**
3. Select scopes:
   - **Public repos**: `public_repo`
   - **Private repos**: `repo` (includes public access)
4. Copy token format: `ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`

### 2. LLM API Keys

#### Anthropic Claude (Recommended)
1. Get API key from https://console.anthropic.com/
2. Format: `sk-ant-xxxxxxxxxxxxxxxxx`
3. Set: `export KIT_ANTHROPIC_TOKEN="your_key"`

#### OpenAI GPT-4
1. Get API key from https://platform.openai.com/
2. Format: `sk-xxxxxxxxxxxxxxxxx`
3. Set: `export KIT_OPENAI_TOKEN="your_key"`

### 3. Configuration

#### Quick Setup
```bash
kit review --init-config
```

This creates `~/.kit/review-config.yaml` with defaults.

#### Manual Configuration

Edit `~/.kit/review-config.yaml`:

```yaml
github:
  token: ghp_your_token_here
  base_url: https://api.github.com

llm:
  provider: anthropic  # or "openai"
  model: claude-sonnet-4-20250514  # or "gpt-4o"
  api_key: sk-ant-your_key_here
  max_tokens: 4000
  temperature: 0.1

review:
  max_files: 50
  post_as_comment: true
  clone_for_analysis: true
  cache_repos: true
  agentic_max_turns: 15
  agentic_finalize_threshold: 10

# Optional: Custom LLM pricing
custom_pricing:
  anthropic:
    claude-sonnet-4-20250514:
      input_per_million: 3.00
      output_per_million: 15.00
  openai:
    gpt-4o:
      input_per_million: 2.50  
      output_per_million: 10.00
```

## ğŸ”§ Usage Examples

### Basic Review
```bash
# Standard mode (recommended)
kit review https://github.com/owner/repo/pull/123

# Dry run (formatted preview without posting)
kit review --dry-run https://github.com/owner/repo/pull/123

# Plain output for piping to other tools
kit review --plain https://github.com/owner/repo/pull/123
kit review -p https://github.com/owner/repo/pull/123

# Override model for specific review
kit review --model gpt-4.1-nano https://github.com/owner/repo/pull/123

# Pipe to Claude Code for implementation
kit review -p https://github.com/owner/repo/pull/123 | \
  claude "Implement all the suggestions from this code review"

# Agentic mode (experimental)
kit review --agentic --agentic-turns 8 https://github.com/owner/repo/pull/123
```

### Cache Management
```bash
# Check cache status
kit review-cache status

# Clean up old cached repos
kit review-cache cleanup

# Clear all cache
kit review-cache clear
```

## ğŸš€ CI/CD Integration

### GitHub Actions Setup

Add automated PR reviews to your repository with GitHub Actions. Kit integrates seamlessly into CI/CD pipelines for consistent, automated code review.

#### Basic Workflow

Create `.github/workflows/pr-review.yml`:

```yaml
name: AI PR Review
on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  ai-review:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read
    
    steps:
      - name: AI Code Review
        run: |
          pip install cased-kit
          kit review ${{ github.event.pull_request.html_url }}
        env:
          KIT_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          KIT_ANTHROPIC_TOKEN: ${{ secrets.ANTHROPIC_API_KEY }}
```

#### Advanced Workflow with Error Handling

```yaml
name: Advanced AI PR Review
on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  ai-review:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read
    
    steps:
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install Kit
        run: |
          pip install --upgrade pip
          pip install cased-kit
          
      - name: Dry Run Review (for testing)
        if: github.event.pull_request.draft
        run: |
          echo "ğŸ” Draft PR - running dry run only"
          kit review --dry-run ${{ github.event.pull_request.html_url }}
        env:
          KIT_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          KIT_ANTHROPIC_TOKEN: ${{ secrets.ANTHROPIC_API_KEY }}
          
      - name: Full AI Review
        if: "!github.event.pull_request.draft"
        run: |
          echo "ğŸ¤– Running AI review for ready PR"
          kit review ${{ github.event.pull_request.html_url }}
        env:
          KIT_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          KIT_ANTHROPIC_TOKEN: ${{ secrets.ANTHROPIC_API_KEY }}
          
      - name: Review Failed
        if: failure()
        run: |
          echo "âŒ AI review failed - check logs for details"
          echo "This might be due to:"
          echo "- Missing API tokens"
          echo "- Rate limiting"
          echo "- Large PR size"
```

#### Budget-Conscious Workflow

```yaml
name: Budget AI PR Review
on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  ai-review:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read
    
    steps:
      - name: Budget AI Review (GPT-4.1-nano)
        run: |
          pip install cased-kit
          
          # Create budget config
          cat > ~/.kit/review-config.yaml << EOF
          github:
            token: env_github_token
          llm:
            provider: openai
            model: gpt-4.1-nano  # Ultra budget: ~$0.005 per PR
            api_key: env_openai_token
          review:
            post_as_comment: true
          EOF
          
          kit review ${{ github.event.pull_request.html_url }}
        env:
          KIT_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          KIT_OPENAI_TOKEN: ${{ secrets.OPENAI_API_KEY }}
```

### Token Setup for CI/CD

#### Required Secrets

Add these secrets to your repository settings (`Settings` â†’ `Secrets and variables` â†’ `Actions`):

| Secret Name | Description | Required |
|-------------|-------------|----------|
| `ANTHROPIC_API_KEY` | Anthropic Claude API key (`sk-ant-...`) | For Claude models |
| `OPENAI_API_KEY` | OpenAI API key (`sk-...`) | For GPT models |

#### GitHub Token

- **GitHub token** is automatically available as `${{ secrets.GITHUB_TOKEN }}`
- Has write permissions to pull requests when `permissions` block is set
- No additional setup required

#### Security Best Practices

```yaml
# âœ… Good: Use organization/repository secrets
env:
  KIT_ANTHROPIC_TOKEN: ${{ secrets.ANTHROPIC_API_KEY }}

# âŒ Bad: Never hardcode tokens
env:
  KIT_ANTHROPIC_TOKEN: "sk-ant-hardcoded-key"

# âœ… Good: Limit permissions
permissions:
  pull-requests: write  # Only what's needed
  contents: read

# âŒ Bad: Excessive permissions
permissions:
  contents: write       # Too broad
```

### Conditional Review Logic

```yaml
# Only review non-draft PRs
- name: AI Review
  if: "!github.event.pull_request.draft"
  
# Only review PRs with specific labels
- name: AI Review
  if: contains(github.event.pull_request.labels.*.name, 'needs-review')
  
# Skip bot PRs
- name: AI Review
  if: "!contains(github.event.pull_request.user.login, 'bot')"
  
# Review only specific file types
- name: Check Changed Files
  id: changes
  uses: dorny/paths-filter@v2
  with:
    filters: |
      code:
        - '**/*.py'
        - '**/*.js'
        - '**/*.ts'
        
- name: AI Review
  if: steps.changes.outputs.code == 'true'
```

### Cost Management in CI

#### Set Review Budgets

```yaml
- name: Cost-Controlled Review
  run: |
    # Use budget model for large PRs
    CHANGED_FILES=$(gh pr view ${{ github.event.pull_request.number }} --json files --jq '.files | length')
    
    if [ $CHANGED_FILES -gt 10 ]; then
      echo "Large PR detected ($CHANGED_FILES files) - using budget model"
      export MODEL="gpt-4.1-nano"  # ~$0.005 per review
    else
      echo "Normal PR size - using recommended model"
      export MODEL="claude-sonnet-4-20250514"  # ~$0.18 per review
    fi
    
    kit review ${{ github.event.pull_request.html_url }}
```

#### Monthly Budget Tracking

```yaml
- name: Budget Tracking
  run: |
    echo "ğŸ’° Expected monthly costs for this repository:"
    echo "- Budget model (GPT-4.1-nano): ~$2.30/month (500 PRs)"
    echo "- Recommended (Claude Sonnet 4): ~$87.95/month (500 PRs)"
    echo "- Premium (Claude Opus 4): ~$454.30/month (500 PRs)"
    
    # You can add actual usage tracking here
    # e.g., send metrics to your monitoring system
```

### Troubleshooting CI Issues

#### Common Problems

```yaml
- name: Debug Review Issues
  if: failure()
  run: |
    echo "ğŸ” Debugging AI review failure..."
    
    # Check token availability
    if [ -z "$KIT_GITHUB_TOKEN" ]; then
      echo "âŒ Missing GitHub token"
    else
      echo "âœ… GitHub token available"
    fi
    
    if [ -z "$KIT_ANTHROPIC_TOKEN" ]; then
      echo "âŒ Missing Anthropic token"
    else
      echo "âœ… Anthropic token available"
    fi
    
    # Check PR accessibility
    echo "ğŸ“‹ PR Details:"
    echo "- Number: ${{ github.event.pull_request.number }}"
    echo "- URL: ${{ github.event.pull_request.html_url }}"
    echo "- Files changed: $(gh pr view ${{ github.event.pull_request.number }} --json files --jq '.files | length')"
```

#### Rate Limiting Handling

```yaml
- name: AI Review with Retry
  uses: nick-invision/retry@v2
  with:
    timeout_minutes: 10
    max_attempts: 3
    retry_on: error
    command: |
      kit review ${{ github.event.pull_request.html_url }}
  env:
    KIT_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    KIT_ANTHROPIC_TOKEN: ${{ secrets.ANTHROPIC_API_KEY }}
```

### Enterprise Considerations

#### Organization-Level Configuration

```yaml
# Use organization secrets for consistent configuration
env:
  KIT_ANTHROPIC_TOKEN: ${{ secrets.ORG_ANTHROPIC_API_KEY }}
  
# Inherit from organization config file
- name: Download Org Config
  run: |
    curl -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
         -o ~/.kit/review-config.yaml \
         https://raw.githubusercontent.com/your-org/kit-config/main/review-config.yaml
```

#### Audit and Compliance

```yaml
- name: Review Audit Log
  run: |
    echo "ğŸ“Š Review completed for PR #${{ github.event.pull_request.number }}"
    echo "- Repository: ${{ github.repository }}"
    echo "- Author: ${{ github.event.pull_request.user.login }}"
    echo "- Timestamp: $(date -u)"
    echo "- Model: claude-sonnet-4-20250514"
    
    # Send to your audit system
    # curl -X POST your-audit-endpoint ...
```

## ğŸ¯ Accuracy Validation

Kit provides several approaches to validate review accuracy:

### Built-in Quality Metrics

Every review includes objective quality scoring:

- **Reference Validation**: Checks if review references actual changed files
- **Line Number Validation**: Verifies line references are plausible
- **Specificity Analysis**: Counts concrete vs vague feedback
- **GitHub Link Validation**: Ensures links point to changed files
- **Content Relevance**: Measures alignment with actual code changes
- **Change Coverage**: Assesses coverage of major modifications

Example output:
```
ğŸ“Š Review Quality Score: 0.75/1.0
ğŸ“ˆ Metrics: {'file_references': 3, 'specific_issues': 5, 'github_links': 4}
```

### Validation Strategies

1. **Comparative Analysis**
   ```bash
   # Compare different modes on same PR
   kit review --dry-run <pr-url>
   kit review --dry-run --agentic <pr-url>
   ```

2. **Historical Tracking**
   - Track quality scores over time
   - Monitor for degradation patterns
   - Compare across different PR types

3. **Spot Checking**
   - Manually validate 10-20% of reviews
   - Focus on high-impact or low-scoring reviews
   - Document common failure patterns

4. **Cross-Validation**
   - Test same PR with different models
   - Compare Claude vs GPT-4 outputs
   - Use multiple team members for validation

Kit includes tools for systematic accuracy validation:

#### Cross-Mode Testing
```bash
# Test a PR across all modes and compare
python -m kit.pr_review.test_accuracy test --pr-url https://github.com/owner/repo/pull/123
```

#### Review Validation
```bash
# Validate an existing review
echo "## Summary\nThis PR adds..." > review.txt
python -m kit.pr_review.test_accuracy validate --pr-url <pr-url> --review-file review.txt
```

#### Regression Testing
```bash
# Test multiple PRs systematically
echo "https://github.com/owner/repo/pull/123" > pr_list.txt
echo "https://github.com/owner/repo/pull/124" >> pr_list.txt
python -m kit.pr_review.test_accuracy regression --pr-list pr_list.txt
```

Output example:
```
ğŸ§ª Testing PR: https://github.com/owner/repo/pull/123
ğŸ› ï¸ STANDARD MODE: $0.15 | 2,856 chars  
ğŸ¤– AGENTIC MODE: $1.25 | 4,102 chars

ğŸ“Š Quality Scores: Standard: 0.84 | Agentic: 0.71*
*Agentic may produce false positives - use with caution
```

## Environment Variables

| Purpose | Variable |
|---------|----------|
| GitHub Token | `KIT_GITHUB_TOKEN` |
| Anthropic Key | `KIT_ANTHROPIC_TOKEN` |
| OpenAI Key | `KIT_OPENAI_TOKEN` |
